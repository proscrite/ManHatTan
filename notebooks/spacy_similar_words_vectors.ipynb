{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "wvec = '/Users/pabloherrero/Downloads/cc.he.300.bin'\n",
    "# with open(wvec, 'rb') as f:\n",
    "#     lines = f.readlines()\n",
    "#     print(len(lines))\n",
    "#     # print(lines[0].decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_path = '/Users/pabloherrero/Documents/ManHatTan/notebooks/he_vectors'\n",
    "nlp = spacy.load(vec_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "זהו True 1.0681664\n",
      "טקסט True 0.8047567\n",
      "בעברית True 0.40524793\n",
      ". True 1.5317025\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"זהו טקסט בעברית.\")\n",
    "for token in doc:\n",
    "    print(token.text, token.has_vector, token.vector_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "חברה True 0.77420473\n",
      "שלי True 1.0418928\n",
      "פיפו True 0.0\n",
      "לא True 1.9327171\n",
      "אדום True 0.83313257\n",
      ". True 1.5317025\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"חברה שלי פיפו לא אדום.\")\n",
    "for token in doc:\n",
    "    print(token.text, token.has_vector, token.vector_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['אדום', 'כחול', 'צהוב', 'ירוק', 'שחור']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adom_id = nlp.vocab.strings['אדום']\n",
    "adom_vec = nlp.vocab.vectors[adom_id]\n",
    "most_similar_words = nlp.vocab.vectors.most_similar(np.asarray([adom_vec]), n=5)\n",
    "words = [nlp.vocab.strings[w] for w in most_similar_words[0][0]]\n",
    "words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bidi.algorithm import get_display\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contained in original:  להגיע\n",
      "Contained in original:  ולהגיע\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['להיכנס', 'לגשת', 'להתקרב']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_word = 'להגיע'\n",
    "# selected_word = get_display(selected_word)\n",
    "selected_word_id = nlp.vocab.strings['להגיע']\n",
    "sel_word_vec = nlp.vocab.vectors[selected_word_id]\n",
    "most_similar_words = nlp.vocab.vectors.most_similar(np.asarray([sel_word_vec]), n=5)\n",
    "words = [nlp.vocab.strings[w] for w in most_similar_words[0][0]]\n",
    "\n",
    "filtered_words = []\n",
    "for w in words:\n",
    "    if selected_word in w:\n",
    "        print('Contained in original: ', w)\n",
    "        continue\n",
    "    filtered_words.append(w)\n",
    "filtered_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['אמרתי', 'ואמרתי', 'אגיד', 'חשבתי', 'ידעתי', 'שאמרתי', 'אומר', 'אמר']"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adom_id = nlp.vocab.strings['אמרתי']\n",
    "adom_vec = nlp.vocab.vectors[adom_id]\n",
    "most_similar_words = nlp.vocab.vectors.most_similar(np.asarray([adom_vec]), n=8)\n",
    "words = [nlp.vocab.strings[w] for w in most_similar_words[0][0]]\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "lippath = '/Users/pabloherrero/Documents/ManHatTan/data/processed/LIPSTICK/hebrew_db.lip'\n",
    "lip = pd.read_csv(lippath)\n",
    "lip.set_index('word_ll', inplace=True, drop=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "עכביש: 1.000, spider\n",
      "קוף: 0.344, monkey\n",
      "מפלצת: 0.316, monster\n",
      "נחש: 0.286, snake\n",
      "יצור: 0.260, creature\n",
      "מכשפה: 0.259, witch\n",
      "קוסם: 0.257, magician\n",
      "מיקרופון: 0.254, microphone\n",
      "טיפש: 0.251, Stupid\n",
      "שריון: 0.251, armor\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load your spaCy model that contains the Hebrew vectors\n",
    "nlp = spacy.load(\"he_vectors\")\n",
    "\n",
    "# Define the target word and process it to get its token (make sure it has a vector)\n",
    "target_word = \"עכביש\"\n",
    "target_token = nlp(target_word)[0]\n",
    "\n",
    "other_words = lip.word_ll.values.tolist()  # List of words to compare with the target word\n",
    "# Compute similarity for each word in the list\n",
    "similarities = []\n",
    "for word in other_words:\n",
    "    token = nlp(word)[0]\n",
    "    # Check if both tokens have vectors\n",
    "    if target_token.has_vector and token.has_vector:\n",
    "        sim = target_token.similarity(token)\n",
    "        similarities.append((word, sim))\n",
    "    else:\n",
    "        similarities.append((word, 0.0))\n",
    "\n",
    "# Sort words by similarity in descending order\n",
    "similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print the top 10 most similar words\n",
    "for word, score in similarities[:10]:\n",
    "    print(f\"{word}: {score:.3f}, {lip.loc[word, 'word_ul']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "288"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(other_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_entries = lip.word_ll.values.tolist()\n",
    "vect_word = []\n",
    "vectors = []\n",
    "for word in all_entries:\n",
    "    token = nlp(word)[0]\n",
    "    # Check if both tokens have vectors\n",
    "    if target_token.has_vector and token.has_vector:\n",
    "        vect_word.append(word)\n",
    "        vectors.append(token.vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathout = '/Users/pabloherrero/Documents/ManHatTan/data/processed/vectors_lip/vectors_heb_lip.npz'\n",
    "\n",
    "np.savez(pathout, tokens=vect_word, vectors=vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load(pathout)\n",
    "tokens = data['tokens']\n",
    "vectors = data['vectors']\n",
    "token_to_vector = {token: vector for token, vector in zip(vect_word, vectors)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "קוף: 0.659, monkey\n",
      "חץ: 0.574, arrow\n",
      "נחש: 0.503, snake\n"
     ]
    }
   ],
   "source": [
    "specific_word = 'עכביש'\n",
    "specific_vector = token_to_vector[specific_word]\n",
    "similarities = []\n",
    "norm = np.dot(specific_vector, specific_vector)\n",
    "for word in tokens:\n",
    "    token = token_to_vector[word]\n",
    "    sim = np.dot(specific_vector, token) / norm\n",
    "    similarities.append((word, sim))\n",
    "similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "# Print the top 10 most similar words\n",
    "for word, score in similarities[1:4]:\n",
    "    print(f\"{word}: {score:.3f}, {lip.loc[word, 'word_ul']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions to sample similar words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import sample\n",
    "\n",
    "def load_similar_words(pathout, target_word):\n",
    "    word_vectors = np.load(pathout)\n",
    "    tokens = word_vectors['tokens']\n",
    "    vectors = word_vectors['vectors']\n",
    "    token_to_vector = {token: vector for token, vector in zip(tokens, vectors)}\n",
    "    try:\n",
    "        target_vector = token_to_vector[target_word]\n",
    "    except IndexError as e:\n",
    "        print(e, f' with {target_word}')\n",
    "        return\n",
    "    similarities = []\n",
    "\n",
    "    norm = np.dot(target_vector, target_vector)\n",
    "    for word in tokens:\n",
    "        token = token_to_vector[word]\n",
    "        sim = np.dot(target_vector, token) / norm\n",
    "        similarities.append((word, sim))\n",
    "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "    # return the top 3 most similar words\n",
    "    return [word for word, _ in similarities[1:10]]\n",
    "\n",
    "def sample_similar_options(lipstick_path : str, iquest : int, modality : str, n_options : int = 3):\n",
    "\n",
    "    if modality == 'dt': word_lang = 'word_ul'\n",
    "    elif modality == 'rt': word_lang = 'word_ll'\n",
    "    else: print('Incorrect modality in rnd_options function')\n",
    "    lip = pd.read_csv(lipstick_path)\n",
    "    target_word = lip.iloc[iquest]['word_ll']\n",
    "    lip = lip.set_index('word_ll', drop=False)\n",
    "    options = {}\n",
    "    #### Change this: to be extracted from lipstick_path:\n",
    "    vector_path = '/Users/pabloherrero/Documents/ManHatTan/data/processed/vectors_lip/vectors_heb_lip.npz'\n",
    "    similar_words = load_similar_words(vector_path, target_word=target_word)\n",
    "    \n",
    "    rnd_similar_words = sample(similar_words, n_options)\n",
    "    print(rnd_similar_words)\n",
    "    for i in range(n_options):\n",
    "        rndOp = lip.loc[rnd_similar_words][word_lang]\n",
    "        print(rndOp)\n",
    "        # options[rndOp] = False\n",
    "    return options\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['קוף', 'חץ', 'נחש', 'חכם', 'פשע', 'פחד', 'תת קרקעי', 'קוסם', 'מה אתה מסתכל']"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_path = '/Users/pabloherrero/Documents/ManHatTan/data/processed/vectors_lip/vectors_heb_lip.npz'\n",
    "load_similar_words(pathout=vector_path, target_word='עכביש')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['נקי', 'מה נראה לך', 'רחב', 'יבש']\n",
      "word_ll\n",
      "נקי                         net\n",
      "מה נראה לך    What do you think\n",
      "רחב                        wide\n",
      "יבש                         dry\n",
      "Name: word_ul, dtype: object\n",
      "word_ll\n",
      "נקי                         net\n",
      "מה נראה לך    What do you think\n",
      "רחב                        wide\n",
      "יבש                         dry\n",
      "Name: word_ul, dtype: object\n",
      "word_ll\n",
      "נקי                         net\n",
      "מה נראה לך    What do you think\n",
      "רחב                        wide\n",
      "יבש                         dry\n",
      "Name: word_ul, dtype: object\n",
      "word_ll\n",
      "נקי                         net\n",
      "מה נראה לך    What do you think\n",
      "רחב                        wide\n",
      "יבש                         dry\n",
      "Name: word_ul, dtype: object\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iquest = 4\n",
    "sample_similar_options(lipstick_path=lippath, iquest=iquest, modality='dt', n_options=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtering out-of-bags words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contained in original:  להגיע\n",
      "Contained in original:  ולהגיע\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['להיכנס', 'לגשת', 'להתקרב']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "selected_word = 'להגיע'\n",
    "# selected_word = get_display(selected_word)\n",
    "selected_word_id = nlp.vocab.strings['להגיע']\n",
    "sel_word_vec = nlp.vocab.vectors[selected_word_id]\n",
    "most_similar_words = nlp.vocab.vectors.most_similar(np.asarray([sel_word_vec]), n=5)\n",
    "words = [nlp.vocab.strings[w] for w in most_similar_words[0][0]]\n",
    "\n",
    "filtered_words = []\n",
    "for w in words:\n",
    "    if selected_word in w:\n",
    "        print('Contained in original: ', w)\n",
    "        continue\n",
    "    filtered_words.append(w)\n",
    "filtered_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_sofits(word):\n",
    "    from hebrew import FINAL_MINOR_LETTER_MAPPINGS as sofit_dict\n",
    "    final_letter = word[-1]\n",
    "    if final_letter in sofit_dict.keys():\n",
    "        filtered_word = word[:-1] + sofit_dict[final_letter]\n",
    "        return filtered_word\n",
    "    else:\n",
    "        return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contained in original:  ציון\n",
      "Contained in original:  הציון\n",
      "Contained in original:  בציון\n",
      "Contained in original:  מציון\n",
      "Contained in original:  בן-ציון\n",
      "Contained in original:  לציון\n",
      "Contained in original:  ציוני\n",
      "Contained in original:  ציונים\n",
      "Contained in original:  הציונים\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['בוויקינתוניםערכים',\n",
       " '1809-443-836החל',\n",
       " 'ממוצע',\n",
       " 'דינור',\n",
       " 'יהונתן',\n",
       " 'משה',\n",
       " 'דוד',\n",
       " 'יובל',\n",
       " 'אברהם',\n",
       " 'הזיתים',\n",
       " 'הלברשטאם']"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_word = 'ציון'\n",
    "# selected_word = get_display(selected_word)\n",
    "selected_word_id = nlp.vocab.strings[selected_word]\n",
    "sel_word_vec = nlp.vocab.vectors[selected_word_id]\n",
    "most_similar_words = nlp.vocab.vectors.most_similar(np.asarray([sel_word_vec]), n=20)\n",
    "words = [nlp.vocab.strings[w] for w in most_similar_words[0][0]]\n",
    "selected_filtered = filter_sofits(selected_word)\n",
    "filtered_words = []\n",
    "for w in words:\n",
    "    wf = filter_sofits(w)\n",
    "    if selected_filtered in wf:\n",
    "        print('Contained in original: ', w)\n",
    "        continue\n",
    "    filtered_words.append(w)\n",
    "filtered_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
